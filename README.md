# Houston-FVI

The scripts in this repository can be used to create a <b>flood vulnerability index (FVI) </b> for Houston, Texas, with ArcGIS software and publicly available data. Flooding is an ongoing problem in Houston, and past storms, such as Hurricane Harvey in 2017, have exposed the logistical challenges such a heavily populated city could face during a major flood. It is important for planners and disaster relief workers to have a comprehensive understanding of flood vulnerability across the city. The FVI scripts have been developed to rank areas of the city by their relative vulnerability and create a GIS layer which can be used to visualize its distribution. In this case flood vulnerability is determined through three major factors: flood risk, shelter accessibility, and social justice. Scores are calculated for each factor, and then a weighted average is calculated from the three scores. The score weights were determined through the analytic hierarchy process (AHP). 

Two main scripts were developed for this analysis. The first script is used to create a social justice dataset from American Community Survey (ACS) estimates, and perform a <b>principal component analysis (PCA)</b> to reduce the dimensionality of the input data. The final social justice score can be calculated from these components. The second script performs the spatial component of the analysis and calculates the final FVI score. Floodplain data, shelter locations, census tract boundaries, tax parcels, land use parcels, and tract-level population estimates are used to determine the distribution of residential parcels, and gain a precise understanding of flood risk across Harris County. This process is referred to as <b>dasymetric mapping</b>. Flood risk and shelter need scores are created through this process, and then then combined with the social justice score for the final index score.

<b>Part 1: Principal Component Analysis</b>

The main purpose of a PCA is to mitigate the effects of <b>multicollinearity</b> within the input dataset. Multiple regression analyses such as this one can produce inaccurate results when several of the independent input variables are highly correlated with each other. Certain variables can predict other variables, making them statistically unreliable. One way to address this issue is through the reduction of dimensionality in the dataset. The purpose of the PCA is to reduce the original number of independent variables to a smaller number of “components”, which are composite scores generated from numerous input variables. Components contain decreasing quantities of information, referred to as <b>variance explained</b>. Only components which contain a significant amount of information can be kept, and factored into the final analysis results.

The PCA script performs most of the necessary steps to create the social justice score. A final step which must be completed manually. Component scores have been “centered” on an axis, so they contain both negative and positive values. The sign of these components is assigned arbitrarily, and may need to be changed based on their correlation with the input dataset. Variables which are highly correlated with a component are considered to be heavily loaded to that component. Variables with a high negative correlation are also heavily loaded, but the sign of the component must be switched to make that loading positive. This step must be performed through the assessment of the component loadings.

This script utilizes the CensusData Python module to access ACS table fields and create the 12 social justice input variables. The social justice dataset is temporarily stored as a Pandas DataFrame. The variables are on different scales (total population, population over 16, households), so they are standardized as percentile rankings. This is accomplished through the SciPy module. The Scikit-Learn module is then used to perform the PCA on the standardized variables. The data in each field is scaled, and then a PCA object is created from it. Twelve components are created for each input variable, but not all will be retained. The<b> Kaiser criterion</b> is applied to the components, in which their eigenvalues are assessed to determine if they make a significant contribution to the results. Components with an eigenvalue greater than 1.00 are kept, and the rest are discarded. From the 12 variables created from the 2018 ACS, only 3 are kept.

The script output consists of two tables: one containing the component scores for each census tract, and the other containing the factor loadings for each component. The loadings can then be used to determine the sign of each component. In the case of this analysis, component 1 was found to have high negative correlation with numerous variables, so its sign was reversed to make the correlation positive. Once modified (if necessary), these components can then be included in the second script, which performs the dasymetric analysis and computes the final index score.

<b>Part Two: Dasymetric Mapping and FVI Calculation</b>

The second script, which performs the spatial analysis and completes the index calculation, is performed primarily through arcpy, although SciPy is also used for percentile calculations and Pandas is utilized for manipulating table data. Tract-level population estimates were disaggregated and redistributed among smaller parcel features. The methodology for this process consisted of classifying residential parcels by specific building codes, which were used to estimate the relative population density of each structure. Once population estimates were assigned to each parcel, they were used to estimate at-risk populations based on their population size and proximity to flood hazard areas. Similarly, at-risk parcels which fell outside of shelter service areas were also identified to determine areas of shelter need.

Two different parcel feature classes are available for Harris County: land use and tax parcels. Although both parcel datasets are classified by different coding systems, they contain the same geometry. Codes for land use parcels are more descriptive for larger residential structures, and those for tax parcels are more descriptive for smaller structures. Features from both of these parcel datasets can be combined in a single dataset, which provides the most descriptive possible set of codes. Parcels are weighted by these codes, and tract-level population estimates are redistributed among each tracts associated parcels according to the weights.

Once population estimates have been assigned to each parcel, they can be used to determine the number of people who could be vulnerable to flooding in each tract. Shelter need is derived from the at-risk parcels. 1-mile service are buffers are created around shelter locations, and populations without shelter access are determined as those at-risk populations which are located outside of those buffers. At-risk populations and shelter-need populations are ranked by percentiles for their respective scores.

Specific weights are assigned for each of the three index scores, which are then combined to calculate the final index score. The weights for the provided index were calculated through an AHP. The three factors were compared against each other, and ranked by relative importance. For this particular index, flood risk was determined to be three times as important as shelter accessibility and twice as important as social justice. Social justice was determined to be three times as important as shelter inaccessibility. These comparisons were input to a excel worksheet created by Klaus Goepel (2013), which is freely available online. From the matrix created from these comparisons, weights were calculated from their normalized eigenvectors. The script was designed so that these weights can be easily changed, for potential sensitivity analyses as well as weights created through other statistical methods.

The index in the map above was created using FEMA’s 100-year floodplain layer as the flood risk boundary. The current floodplain map has been found to underestimate  flood potential in certain areas, although new floodplain maps are expected to be available in 2021, which will correct these inaccuracies. This script was developed with the changing nature of Houston’s flood risk landscape in mind.  The input workspace, feature classes, tables, and code fields can be easily changed and manipulated at the top of the provided python file.

